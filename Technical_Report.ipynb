{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/stroke-data.csv\n",
      "length before 'NA' values removed: 5110\n",
      "length after 'NA' values removed: 4909\n",
      "amount of non-strokes with no 'NA' rows: 4700\n",
      "length of downsized non-stroke data: 1000\n",
      "length of all downsized data: 1209\n",
      "length of final_data 1209\n",
      "non-stroke amount: 1000\n",
      "stroke amount: 209\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "import mysklearn.mypytable\n",
    "import os\n",
    "import copy\n",
    "\n",
    "filename = os.path.join(\"input_data\", \"stroke-data.csv\")\n",
    "print(filename)\n",
    "table = mysklearn.mypytable.MyPyTable().load_from_file(filename)\n",
    "print(\"length before 'NA' values removed:\", len(table.data))\n",
    "\n",
    "table.remove_rows_with_missing_values()\n",
    "print(\"length after 'NA' values removed:\", len(table.data))\n",
    "\n",
    "# creating downsample data\n",
    "table_deep_copy = copy.deepcopy(table)\n",
    "\n",
    "non_stroke = []\n",
    "for row in table.data:\n",
    "    if row[-1] == 0.0:\n",
    "        non_stroke.append(row)\n",
    "print(\"amount of non-strokes with no 'NA' rows:\", len(non_stroke))\n",
    "\n",
    "downsized_non_stroke_data = []\n",
    "for i in range(0, 1000):\n",
    "    index = np.random.randint(0, len(non_stroke))\n",
    "    row = non_stroke[index]\n",
    "    downsized_non_stroke_data.append(row)\n",
    "    non_stroke.remove(row)\n",
    "\n",
    "print(\"length of downsized non-stroke data:\", len(downsized_non_stroke_data))\n",
    "\n",
    "final_data_downsized = []\n",
    "for row in table_deep_copy.data:\n",
    "    if row[-1] == 1.0:\n",
    "        final_data_downsized.append(row)\n",
    "\n",
    "final_data_downsized = final_data_downsized + downsized_non_stroke_data # adding 1.0 class label and downsized 0.0 sample\n",
    "\n",
    "print(\"length of all downsized data:\", len(final_data_downsized)) # 1000 0 class labels, 209 1 class labels (after removal of 'NA' rows)\n",
    "\n",
    "final_data = table_deep_copy\n",
    "final_data.data = final_data_downsized\n",
    "print(\"length of final_data\", len(final_data.data))\n",
    "\n",
    "final_data.save_to_file(\"input_data/stroke-data-downsized.csv\")\n",
    "\n",
    "stroke_count = 0\n",
    "non_stroke_count = 0\n",
    "for row in final_data.data:\n",
    "    if row[-1] == 0.0:\n",
    "        non_stroke_count += 1\n",
    "    elif row[-1] == 1.0:\n",
    "        stroke_count += 1\n",
    "\n",
    "print(\"non-stroke amount:\", non_stroke_count)\n",
    "print(\"stroke amount:\", stroke_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
